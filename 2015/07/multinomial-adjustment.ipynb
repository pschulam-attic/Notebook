{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import imp\n",
    "sys.path.append('/Users/pschulam/Git/mypy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nips15\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn import preprocessing, linear_model, cross_validation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from mypy.models import softmax\n",
    "_ = imp.reload(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PatientData(tbl, t, y, x1, x2):\n",
    "    pd = {}\n",
    "    pd['ptid'] = int(tbl['ptid'].values[0])\n",
    "    pd['t'] = tbl[t].values.copy()\n",
    "    pd['t'] = np.array([]) if np.all(np.isnan(pd['t'])) else pd['t']\n",
    "    pd['y'] = tbl[y].values.copy()\n",
    "    pd['y'] = np.array([]) if np.all(np.isnan(pd['y'])) else pd['y']\n",
    "    pd['x1'] = np.asarray(tbl.loc[:, x1].drop_duplicates()).ravel()\n",
    "    pd['x2'] = np.asarray(tbl.loc[:, x2].drop_duplicates()).ravel()\n",
    "    pd['x2'] = np.r_[1.0, pd['x2']]\n",
    "    return pd\n",
    "        \n",
    "def truncated_data(pd, censor_time):\n",
    "    obs = pd['t'] <= censor_time\n",
    "    pdc = deepcopy(pd)\n",
    "    pdc['t'] = pd['t'][obs]\n",
    "    pdc['y'] = pd['y'][obs]\n",
    "    return pdc, pd['t'][~obs]\n",
    "\n",
    "def unpack_data(patient_data):\n",
    "    t = patient_data['t']\n",
    "    x1 = patient_data['x1']\n",
    "    x2 = patient_data['x2']\n",
    "    y = patient_data['y']\n",
    "    return t, x1, x2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pfvc_spec = {'t' : 'years_seen_full',\n",
    "             'y' : 'pfvc',\n",
    "             'x1': ['female', 'afram'],\n",
    "             'x2': ['female', 'afram', 'aca', 'scl']}\n",
    "\n",
    "pfvc    = pd.read_csv('data/benchmark_pfvc.csv')\n",
    "pfvc_pd = [PatientData(tbl, **pfvc_spec) for _, tbl in pfvc.groupby('ptid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tss_spec = {'t' : 'years_seen',\n",
    "            'y' : 'tss',\n",
    "            'x1': ['female', 'afram'],\n",
    "            'x2': ['female', 'afram']}\n",
    "\n",
    "tss = pd.read_csv('data/benchmark_tss.csv')\n",
    "tss_match = ['ptid'] + tss_spec['x1']\n",
    "tss = pd.merge(pfvc[tss_match], tss, 'left', tss_match)\n",
    "tss_pd = [PatientData(tbl, **tss_spec) for _, tbl in tss.groupby('ptid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdlco_spec = {'t' : 'years_seen',\n",
    "              'y' : 'pdlco',\n",
    "              'x1': ['female', 'afram'],\n",
    "              'x2': ['female', 'afram']}\n",
    "\n",
    "pdlco = pd.read_csv('data/benchmark_pdc.csv')\n",
    "pdlco_match = ['ptid'] + pdlco_spec['x1']\n",
    "pdlco = pd.merge(pfvc[pdlco_match], pdlco, 'left', pdlco_match)\n",
    "pdlco_pd = [PatientData(tbl, **pdlco_spec) for _, tbl in pdlco.groupby('ptid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv1_spec = {'t' : 'years_seen',\n",
    "            'y' : 'pfev1',\n",
    "            'x1': ['female', 'afram'],\n",
    "            'x2': ['female', 'afram']}\n",
    "\n",
    "pv1 = pd.read_csv('data/benchmark_pv1.csv')\n",
    "pv1_match = ['ptid'] + pv1_spec['x1']\n",
    "pv1 = pd.merge(pfvc[pv1_match], pv1, 'left', pv1_match)\n",
    "pv1_pd = [PatientData(tbl, **pv1_spec) for _, tbl in pv1.groupby('ptid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ef_spec = {'t' : 'years_seen',\n",
    "           'y' : 'ef',\n",
    "           'x1': ['female', 'afram'],\n",
    "           'x2': ['female', 'afram']}\n",
    "\n",
    "ef = pd.read_csv('data/benchmark_ef.csv')\n",
    "ef_match = ['ptid'] + ef_spec['x1']\n",
    "ef = pd.merge(pfvc[ef_match], ef, 'left', ef_match)\n",
    "ef_pd = [PatientData(tbl, **ef_spec) for _, tbl in ef.groupby('ptid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_pd = {\n",
    "    'pfvc' : pfvc_pd,\n",
    "    'tss'  : tss_pd,\n",
    "    'pdlco': pdlco_pd,\n",
    "    'pv1'  : pv1_pd,\n",
    "    'ef'   : ef_pd\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pfvc_model  = nips15.NipsModel.from_directory('models/pfvc')\n",
    "tss_model   = nips15.NipsModel.from_directory('models/tss')\n",
    "pdlco_model = nips15.NipsModel.from_directory('models/pdc')\n",
    "pv1_model   = nips15.NipsModel.from_directory('models/pv1')\n",
    "ef_model    = nips15.NipsModel.from_directory('models/ef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_models = {\n",
    "    'pfvc' : pfvc_model,\n",
    "    'tss'  : tss_model,\n",
    "    'pdlco': pdlco_model,\n",
    "    'pv1'  : pv1_model,\n",
    "    'ef'   : ef_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_problem(target, aux, censor, sig_censor, feat_censor, patient_data, models):\n",
    "    P = np.array([models[target].posterior(*unpack_data(d)) for d in patient_data[target]])\n",
    "    Q = np.array([models[target].posterior(*unpack_data(truncated_data(d, censor)[0])) for d in patient_data[target]])\n",
    "    \n",
    "    Q_sig = []\n",
    "    for marker in aux:\n",
    "        Qi = np.array([models[marker].posterior(*unpack_data(truncated_data(d, sig_censor)[0]))\n",
    "                       for d in patient_data[marker]])\n",
    "        Q_sig.append(Qi)\n",
    "\n",
    "    Q_feat = []\n",
    "    for marker in aux:\n",
    "        Qi = np.array([models[marker].posterior(*unpack_data(truncated_data(d, feat_censor)[0]))\n",
    "                       for d in patient_data[marker]])\n",
    "        Q_feat.append(Qi)\n",
    "    \n",
    "    S = [check_significance(Qi, np.argmax(P, axis=1), P.shape[1]) for Qi in Q_sig]\n",
    "    M = np.concatenate(S, axis=1)\n",
    "    \n",
    "    X = np.concatenate(Q_feat, axis=1)\n",
    "    \n",
    "    return P, Q, X, M\n",
    "\n",
    "def fit(P, Q, X, M):\n",
    "    k  = P.shape[1]\n",
    "    d  = X.shape[1]\n",
    "    W0 = np.zeros((k, d))\n",
    "    C  = offset(Q)\n",
    "    \n",
    "    def f(w):\n",
    "        W = w.reshape(W0.shape)\n",
    "        y = [softmax.regression_ll(x, y, W, c) for x, y, c in zip(X, P, C)]\n",
    "        return -sum(y)\n",
    "    \n",
    "    def g(w):\n",
    "        W = w.reshape(W0.shape)\n",
    "        y = [softmax.regression_ll_grad(x, y, W, c) for x, y, c in zip(X, P, C)]\n",
    "        y = -sum(y)\n",
    "        y[~M] = 0.0\n",
    "        return y.ravel()\n",
    "    \n",
    "    s = minimize(f, W0.ravel(), jac=g, method='BFGS')\n",
    "    W = s.x.reshape(W0.shape)\n",
    "    Qhat = np.array([softmax.regression_proba(x, W, c) for x, c in zip(X, C)])\n",
    "    \n",
    "    return Qhat, W, C, s\n",
    "\n",
    "def fitcv(nfolds, P, Q, X, M):\n",
    "    Qhat  = np.zeros_like(Q)\n",
    "    folds = cross_validation.KFold(P.shape[0], nfolds, shuffle=True, random_state=0)\n",
    "    W     = []\n",
    "    C     = offset(Q)\n",
    "    s     = []\n",
    "    \n",
    "    for i, (train, test) in enumerate(folds):\n",
    "        print('Fitting fold {}.'.format(i))\n",
    "        Qhat_i, Wi, _, si = fit(P[train], Q[train], X[train], M)\n",
    "        if not si.success:\n",
    "            print('Warning: optimization did not terminate:\\n{}'.format(si.message))\n",
    "            \n",
    "        W.append(Wi)\n",
    "        s.append(si)\n",
    "        \n",
    "        Qhat[test] = np.array([softmax.regression_proba(x, Wi, c) for x, c in zip(X[test], C[test])])\n",
    "        \n",
    "    return Qhat, W, C, s\n",
    "\n",
    "def offset(Q):\n",
    "    return np.log(Q) - np.log(Q[:, 0][:, np.newaxis])\n",
    "\n",
    "def xentropy(P, Q=None):\n",
    "    if Q is None:\n",
    "        Q = P\n",
    "    return - np.sum(P * np.log(Q))\n",
    "\n",
    "def check_significance(Q, z, k):\n",
    "    pvalues     = np.zeros((k, Q.shape[1]))\n",
    "    significant = np.zeros((k, Q.shape[1]), dtype=np.bool)\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        c1 = est_bernoullis(Q[z == 0])\n",
    "        c2 = est_bernoullis(Q[z == i])\n",
    "        d  = c2[0] - c1[0]\n",
    "        s  = np.sqrt(c1[2] ** 2 + c2[2] ** 2)\n",
    "        pvalues[i] = np.abs(d / s)\n",
    "        significant[i] = pvalues[i] >= 2.0\n",
    "\n",
    "    return significant\n",
    "\n",
    "def est_bernoullis(Q):\n",
    "    n = Q.sum()\n",
    "    p = Q.sum(axis=0) / n\n",
    "    v = p * (1 - p)\n",
    "    s = np.sqrt(v / n)\n",
    "    return p, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_cheat = make_problem('pfvc', ['tss', 'pdlco', 'pv1', 'ef'], 1.0, float('inf'), float('inf'), all_pd, all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sig_cheat  = make_problem('pfvc', ['tss', 'pdlco', 'pv1', 'ef'], 1.0, float('inf'), 1.0, all_pd, all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_cheat = make_problem('pfvc', ['tss', 'pdlco', 'pv1', 'ef'], 1.0, 1.0, float('inf'), all_pd, all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "honest     = make_problem('pfvc', ['tss', 'pdlco', 'pv1', 'ef'], 1.0, 1.0, 1.0, all_pd, all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629.49656310072237"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_cheat_fit = fit(*full_cheat)\n",
    "xentropy(full_cheat[0], full_cheat_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654.56849879937954"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_cheat_fit_cv = fitcv(20, *full_cheat)\n",
    "xentropy(full_cheat[0], full_cheat_fit_cv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775.21876480539697"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_cheat_fit  = fit(*sig_cheat)\n",
    "xentropy(sig_cheat[0], sig_cheat_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold 0.\n",
      "Fitting fold 1.\n",
      "Fitting fold 2.\n",
      "Fitting fold 3.\n",
      "Fitting fold 4.\n",
      "Fitting fold 5.\n",
      "Fitting fold 6.\n",
      "Fitting fold 7.\n",
      "Fitting fold 8.\n",
      "Fitting fold 9.\n",
      "Fitting fold 10.\n",
      "Fitting fold 11.\n",
      "Fitting fold 12.\n",
      "Fitting fold 13.\n",
      "Fitting fold 14.\n",
      "Fitting fold 15.\n",
      "Fitting fold 16.\n",
      "Fitting fold 17.\n",
      "Fitting fold 18.\n",
      "Fitting fold 19.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "809.84875809660787"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_cheat_fit_cv = fitcv(20, *sig_cheat)\n",
    "xentropy(sig_cheat[0], sig_cheat_fit_cv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655.98553066978707"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cheat_fit = fit(*feat_cheat)\n",
    "xentropy(feat_cheat[0], feat_cheat_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold 0.\n",
      "Fitting fold 1.\n",
      "Warning: optimization did not terminate:\n",
      "Desired error not necessarily achieved due to precision loss.\n",
      "Fitting fold 2.\n",
      "Fitting fold 3.\n",
      "Fitting fold 4.\n",
      "Fitting fold 5.\n",
      "Fitting fold 6.\n",
      "Fitting fold 7.\n",
      "Fitting fold 8.\n",
      "Fitting fold 9.\n",
      "Fitting fold 10.\n",
      "Fitting fold 11.\n",
      "Fitting fold 12.\n",
      "Fitting fold 13.\n",
      "Fitting fold 14.\n",
      "Fitting fold 15.\n",
      "Fitting fold 16.\n",
      "Fitting fold 17.\n",
      "Fitting fold 18.\n",
      "Fitting fold 19.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "670.50080824503016"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cheat_fit_cv = fitcv(20, *feat_cheat)\n",
    "xentropy(feat_cheat[0], feat_cheat_fit_cv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00],\n",
       "       [  0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,  -3.959e-01,  -7.424e-01,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00],\n",
       "       [  0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "         -3.964e-01,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,  -1.021e+00,   9.089e-01,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00],\n",
       "       [  0.000e+00,   0.000e+00,   0.000e+00,   3.941e-02,   0.000e+00,\n",
       "         -7.599e-01,   0.000e+00,   5.849e-02,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,  -3.366e+00,   1.780e+00,  -1.013e+00,\n",
       "          0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00],\n",
       "       [  0.000e+00,   0.000e+00,   0.000e+00,   9.958e-02,   0.000e+00,\n",
       "         -3.500e-01,   0.000e+00,   2.513e-01,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   0.000e+00,  -1.146e+00,   0.000e+00,   1.475e-01,\n",
       "         -1.131e+00,   0.000e+00,   0.000e+00,   0.000e+00],\n",
       "       [  0.000e+00,   0.000e+00,   0.000e+00,   3.443e-01,   0.000e+00,\n",
       "         -3.436e-01,   0.000e+00,   7.142e-01,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   6.137e-01,  -7.133e+00,   0.000e+00,  -6.332e-01,\n",
       "          8.835e-01,   0.000e+00,   0.000e+00,   0.000e+00],\n",
       "       [  0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "         -8.063e-01,   0.000e+00,   2.319e-01,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   4.382e-01,  -3.588e+00,   0.000e+00,  -2.750e+00,\n",
       "          2.117e+00,   0.000e+00,   0.000e+00,   0.000e+00],\n",
       "       [  0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,  -1.090e+01,\n",
       "          4.528e-01,   0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00,\n",
       "          0.000e+00,   6.329e-01,  -6.442e+02,   0.000e+00,  -1.756e+00,\n",
       "          1.979e+00,   0.000e+00,   0.000e+00,   0.000e+00]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cheat_fit_cv[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subtypes = pd.read_csv('benchmark_pfvc_subtypes.csv')\n",
    "subtypes['subtype'] = np.argmax(feat_cheat_fit_cv[0], axis=1) + 1\n",
    "subtypes.to_csv('benchmark_pfvc_1y_subtypes_feat_cheat_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782.16033097543652"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honest_fit     = fit(*honest)\n",
    "xentropy(honest[0], honest_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold 0.\n",
      "Fitting fold 1.\n",
      "Fitting fold 2.\n",
      "Fitting fold 3.\n",
      "Fitting fold 4.\n",
      "Fitting fold 5.\n",
      "Fitting fold 6.\n",
      "Fitting fold 7.\n",
      "Fitting fold 8.\n",
      "Fitting fold 9.\n",
      "Fitting fold 10.\n",
      "Fitting fold 11.\n",
      "Fitting fold 12.\n",
      "Fitting fold 13.\n",
      "Fitting fold 14.\n",
      "Fitting fold 15.\n",
      "Fitting fold 16.\n",
      "Fitting fold 17.\n",
      "Fitting fold 18.\n",
      "Fitting fold 19.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "806.38916857559275"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honest_fit_cv  = fitcv(20, *honest)\n",
    "xentropy(honest[0], honest_fit_cv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.81, ...,  0.01,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.39,  0.53, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.6 ,  0.38, ...,  0.  ,  0.  ,  0.  ],\n",
       "       ..., \n",
       "       [ 0.  ,  0.  ,  0.02, ...,  0.23,  0.05,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.84,  0.04,  0.09],\n",
       "       [ 0.  ,  0.  ,  0.01, ...,  0.09,  0.02,  0.  ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(honest[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Using full information (`full_cheat`) yields the best training and cross-validated loss. The second best is the feature cheating case (`feat_cheat`). Why is this? It appears that knowing the true posterior probabilities is important, but there is another factor to consider: can we estimate the right features to use accurately? It turns our that the `feat_cheat` feature selection is *approximately* a subset of the `sig_cheat` features. In other words, it misses out on the opportunity to use some of the useful features, but does not include very many irrelevant ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90,  3],\n",
       "       [26, 33]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(sig_cheat[-1].ravel(), feat_cheat[-1].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we take away from this? Estimating the subtype memberships correctly is the most important thing. Are there ways to do this outside of simply observing more data? Not that I can think of, so maybe the way to analyze is to simply see how using the online training helps to make predictions accurate more quickly than if we used the PFVC data alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
