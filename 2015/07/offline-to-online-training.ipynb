{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline-to-Online Training\n",
    "\n",
    "Our model is trained generatively---the observed data log-likelihood is maximized using the EM algorithm. However, our goal is to deploy the model in a predictive setting. We want to predict the most likely future trajectory given (1) any baseline information and (2) the noisy marker values observed so far. The focus of this notebook is to understand how we can adjust the parameters of a generative trajectory model in order to improve the performance on the trajectory prediction task.\n",
    "\n",
    "## Related Work\n",
    "\n",
    "The paper by [Raina and Ng (2003)](http://ai.stanford.edu/~rajatr/papers/nips03-hybrid.pdf) describes a hybrid generative/discriminative model. One of the key ideas in this work is the relative importance of random variables in the generative model when applied in a predictive context (i.e. the generative model is used to derive a conditional probability through Bayes rule). On page 3 there is an interesting point: they show that the decision rule for binary classification of UseNet documents can be formulated as a comparison between the sum of log-likelihood terms. They note that if features are extracted from, say, the message title and the message body there are many more log-likelihood terms for the body than there are for the title. The title, however, may be informative for making the decision. The NBC (or more generally any generatively trained classifier) will treat them all equally, however.\n",
    "\n",
    "## Experimental Setup\n",
    "\n",
    "The metric of interest is the mean absolute error aggregated within the usual buckets we've defined---(1,2], (2,4], (4,8], and (8,25]. We'll begin by looking at predictions made after observing one year of data (i.e. we will only train a single online-adapted model). We will compare to two baselines. The first baseline will be the predictions made using the MAP estimate of the subtype under full information (i.e. observing all of the individual's pFVC data) and the second baseline will be the MAP estimate of the subtype under one year of data (i.e. the standard conditional prediction obtained via application of Bayes rule to the generative model).\n",
    "\n",
    "## Methods\n",
    "\n",
    "For each individual $i$, let $y_i$ denote the vector of observed measurements, $t_i$ the measurement times, and $x_i$ the vector of covariates used in the population and subpopulation models. Each individual is associated with a subtype, which we denote using $z_i \\in \\{1, \\ldots, K\\}$. Let $\\Phi_1(t_i)$ denote the population feature matrix, $\\Phi_2(t_i)$ denote the subpopulation feature matrix, and $\\Phi_3(t_i)$ denote the individual-specific long-term effects feature matrix.\n",
    "\n",
    "In the generative model, we specify the marginal probability of subtype membership and the conditional probability of observed markers given subtype membership. The marginal probability of subtype membership is modeled using softmax multiclass regression:\n",
    "\n",
    "$$ p(z_i = k \\mid w_{1:K}) \\propto \\exp \\{ x_i^\\top w_k \\}. $$\n",
    "\n",
    "The conditional probability of a marker sequence given subtype membership is\n",
    "\n",
    "$$ p(y_i \\mid z_i = k, \\beta_{1:K}) = \\mathcal{N} ( m_i(k), \\Sigma_i ), $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ m_i(k) = \\Phi_1(t_i) \\Lambda x_i + \\Phi_2(t_i) \\beta_k $$\n",
    "and\n",
    "$$ \\Sigma_i = \\Phi_3(t_i) \\Sigma_b \\Phi_3^\\top(t_i) + K_{\\text{OU}}(t_i) + \\sigma^2 \\mathbf{1}. $$\n",
    "\n",
    "Given some observed data $y_i$, the posterior over subtype membership $z_i$ is\n",
    "\n",
    "$$ p(z_i = k \\mid y_i) \\propto p(z_i = k \\mid w_{1:K}) p(y_i \\mid z_i = k, \\beta_{1:K}). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/pschulam/Git/mypy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-spline Basis\n",
    "\n",
    "This basis is **hard-coded** to implement the exact basis functions used to fit the model in the R code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mypy import bsplines\n",
    "\n",
    "boundaries   = (-1.0, 23.0)\n",
    "degree       = 2\n",
    "num_features = 6\n",
    "basis = bsplines.universal_basis(boundaries, degree, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mypy.util import as_row, as_col\n",
    "\n",
    "def kernel(x1, x2=None, a_const=1.0, a_ou=1.0, l_ou=1.0):\n",
    "    symmetric = x2 is None\n",
    "    d = differences(x1, x1) if symmetric else differences(x1, x2)\n",
    "    K = a_const * np.ones_like(d)\n",
    "    K += ou_kernel(d, a_ou, l_ou)\n",
    "    if symmetric:\n",
    "        K += np.eye(x1.size)\n",
    "    return K\n",
    "\n",
    "def ou_kernel(d, a, l):\n",
    "    return a * np.exp( - np.abs(d) / l )\n",
    "\n",
    "def differences(x1, x2):\n",
    "    return as_col(x1) - as_row(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.linspace(0, 20, 41)\n",
    "X_test = basis.eval(x_test)\n",
    "K_test = kernel(x_test, a_const=16.0, a_ou=36.0, l_ou=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6944,  0.2917,  0.0139,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.5625,  0.4062,  0.0312,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.4444,  0.5   ,  0.0556,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.3403,  0.5729,  0.0868,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.25  ,  0.625 ,  0.125 ,  0.    ,  0.    ,  0.    ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 53.    ,  44.0368,  37.8351,  33.0052,  29.2437],\n",
       "       [ 44.0368,  53.    ,  44.0368,  37.8351,  33.0052],\n",
       "       [ 37.8351,  44.0368,  53.    ,  44.0368,  37.8351],\n",
       "       [ 33.0052,  37.8351,  44.0368,  53.    ,  44.0368],\n",
       "       [ 29.2437,  33.0052,  37.8351,  44.0368,  53.    ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_test[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mypy.models.softmax' from '/Users/pschulam/Git/mypy/mypy/models/softmax.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "from mypy.models import softmax\n",
    "reload(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "a_const = 16.0\n",
    "a_ou    = 36.0\n",
    "l_ou    = 2.0\n",
    "\n",
    "def phi1(x):\n",
    "    return np.ones((x.size, 1))\n",
    "\n",
    "def phi2(x):\n",
    "    return basis.eval(x)\n",
    "\n",
    "def gp_posterior(tnew, t, y, kern, **kwargs):\n",
    "    from numpy import dot\n",
    "    from scipy.linalg import inv, solve\n",
    "    \n",
    "    K11 = kern(tnew, **kwargs)\n",
    "    K12 = kern(tnew, t, **kwargs)\n",
    "    K22 = kern(t, **kwargs)\n",
    "    \n",
    "    m = dot(K12, solve(K22, y))\n",
    "    K = K11 - dot(K12, solve(K22, K12.T))\n",
    "    \n",
    "    return m, K\n",
    "\n",
    "def trajectory_means(t, x, b, B):\n",
    "    from numpy import dot\n",
    "    \n",
    "    P1 = phi1(t)\n",
    "    P2 = phi2(t)\n",
    "    \n",
    "    m1 = dot(P1, dot(b, x)).ravel()\n",
    "    m2 = dot(B, P2.T)\n",
    "    \n",
    "    return m1 + m2\n",
    "\n",
    "def trajectory_logl(t, x, y, z, B, b):\n",
    "    if t.size < 1:\n",
    "        return 0.0\n",
    "    \n",
    "    m = trajectory_means(t, x, b, B)[z]\n",
    "    S = kernel(t, a_const=a_const, a_ou=a_ou, l_ou=l_ou)\n",
    "    \n",
    "    return multivariate_normal.logpdf(y, m, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.loadtxt('param/pop.dat')\n",
    "B = np.loadtxt('param/subpop.dat')\n",
    "W = np.loadtxt('param/marginal.dat')\n",
    "W = np.r_[ np.zeros((1, W.shape[1])), W ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "\n",
    "def model_posterior(t, x1, x2, y, b, B, W):\n",
    "    k = B.shape[0]\n",
    "    lp = softmax.regression_log_proba(x2, W)\n",
    "    lp += np.array([trajectory_logl(t, x1, y, z, B, b) for z in range(k)])\n",
    "    ll = logsumexp(lp)\n",
    "    return np.exp(lp - ll)\n",
    "\n",
    "def model_loglik(t, x1, x2, y, b, B, W):\n",
    "    k = B.shape[0]\n",
    "    lp = softmax.regression_log_proba(x2, W)\n",
    "    lp += np.array([trajectory_logl(t, x1, y, z, B, b) for z in range(k)])\n",
    "    ll = logsumexp(lp)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def PatientData(tbl):\n",
    "    pd = {}\n",
    "    pd['ptid'] = int(tbl['ptid'].values[0])\n",
    "    pd['t']    = tbl['years_seen_full'].values.copy()\n",
    "    pd['y']    = tbl['pfvc'].values.copy()\n",
    "    pd['x1']   = np.asarray(tbl.loc[:, ['female', 'afram']].drop_duplicates()).ravel()\n",
    "    pd['x2']   = np.asarray(tbl.loc[:, ['female', 'afram', 'aca', 'scl']].drop_duplicates()).ravel()\n",
    "    pd['x2']   = np.r_[ 1.0, pd['x2'] ]\n",
    "    return pd\n",
    "\n",
    "def truncated_data(pd, censor_time):\n",
    "    obs = pd['t'] <= censor_time\n",
    "    pdc = deepcopy(pd)\n",
    "    pdc['t'] = pd['t'][obs]\n",
    "    pdc['y'] = pd['y'][obs]\n",
    "    return pdc, pd['t'][~obs]\n",
    "\n",
    "def run_inference(pd, b, B, W):\n",
    "    ll = model_loglik(pd['t'], pd['x1'], pd['x2'], pd['y'], b, B, W)\n",
    "    posterior = model_posterior(pd['t'], pd['x1'], pd['x2'], pd['y'], b, B, W)\n",
    "    return ll, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pfvc = pd.read_csv('data/benchmark_pfvc.csv')\n",
    "data = [PatientData(tbl) for _, tbl in pfvc.groupby('ptid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.   ,  0.   ,  0.   ,  0.01 ,  0.022,  0.117,  0.816,  0.035])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll, pst = run_inference(data[9], b, B, W)\n",
    "np.round(pst, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Tuning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_posteriors = np.array([run_inference(d, b, B, W)[1] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_posteriors = np.array([run_inference(truncated_data(d, 1.0)[0], b, B, W)[1] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-430.77831436230167"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(full_posteriors * np.log(full_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-790.52486194944299"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(full_posteriors * np.log(part_posteriors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting to the Full Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.    ,   0.    ,   0.    ,   0.    ,   0.    ],\n",
       "       [  1.5932,  -0.539 ,  12.3583,   0.8797,   1.1137],\n",
       "       [  1.8355,  -0.2656,  11.9278,  -0.8695,  -0.3211],\n",
       "       [  1.8279,  -1.    ,  12.8038,  -0.5704,   1.7303],\n",
       "       [  1.7253,  -0.6034,  13.1681,  -0.925 ,   1.2165],\n",
       "       [ -0.0353,   0.3386,  13.7896,  -1.5672,   1.7802],\n",
       "       [  1.9339,  -1.4838,  12.6736,  -0.9307,   1.2987],\n",
       "       [  0.5535,  -1.1074,  12.6253, -16.2041,   1.1487]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([d['x2'] for d in data])\n",
    "Y = full_posteriors.copy()\n",
    "k, p = W.shape\n",
    "\n",
    "def f(w, k=k, p=p, X=X, Y=Y):\n",
    "    W = w.reshape((k, p))\n",
    "    return -sum(softmax.regression_ll(xi, yi, W) for xi, yi in zip(X, Y))\n",
    "\n",
    "def g(w, k=k, p=p, X=X, Y=Y):\n",
    "    W = w.reshape((k, p))\n",
    "    G = sum(softmax.regression_ll_grad(xi, yi, W) for xi, yi in zip(X, Y))\n",
    "    return -G.ravel()\n",
    "\n",
    "solution = opt.minimize(f, W.ravel(), jac=g, method='BFGS')\n",
    "solution.x.reshape((k, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215.1193204026597"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution.fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
