{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline-to-Online Training\n",
    "\n",
    "Our model is trained generatively---the observed data log-likelihood is maximized using the EM algorithm. However, our goal is to deploy the model in a predictive setting. We want to predict the most likely future trajectory given (1) any baseline information and (2) the noisy marker values observed so far. The focus of this notebook is to understand how we can adjust the parameters of a generative trajectory model in order to improve the performance on the trajectory prediction task.\n",
    "\n",
    "## Related Work\n",
    "\n",
    "The paper by [Raina and Ng (2003)](http://ai.stanford.edu/~rajatr/papers/nips03-hybrid.pdf) describes a hybrid generative/discriminative model. One of the key ideas in this work is the relative importance of random variables in the generative model when applied in a predictive context (i.e. the generative model is used to derive a conditional probability through Bayes rule). On page 3 there is an interesting point: they show that the decision rule for binary classification of UseNet documents can be formulated as a comparison between the sum of log-likelihood terms. They note that if features are extracted from, say, the message title and the message body there are many more log-likelihood terms for the body than there are for the title. The title, however, may be informative for making the decision. The NBC (or more generally any generatively trained classifier) will treat them all equally, however.\n",
    "\n",
    "## Experimental Setup\n",
    "\n",
    "The metric of interest is the mean absolute error aggregated within the usual buckets we've defined---(1,2], (2,4], (4,8], and (8,25]. We'll begin by looking at predictions made after observing one year of data (i.e. we will only train a single online-adapted model). We will compare to two baselines. The first baseline will be the predictions made using the MAP estimate of the subtype under full information (i.e. observing all of the individual's pFVC data) and the second baseline will be the MAP estimate of the subtype under one year of data (i.e. the standard conditional prediction obtained via application of Bayes rule to the generative model).\n",
    "\n",
    "## Methods\n",
    "\n",
    "For each individual $i$, let $y_i$ denote the vector of observed measurements, $t_i$ the measurement times, and $x_i$ the vector of covariates used in the population and subpopulation models. Each individual is associated with a subtype, which we denote using $z_i \\in \\{1, \\ldots, K\\}$. Let $\\Phi_1(t_i)$ denote the population feature matrix, $\\Phi_2(t_i)$ denote the subpopulation feature matrix, and $\\Phi_3(t_i)$ denote the individual-specific long-term effects feature matrix.\n",
    "\n",
    "In the generative model, we specify the marginal probability of subtype membership and the conditional probability of observed markers given subtype membership. The marginal probability of subtype membership is modeled using softmax multiclass regression:\n",
    "\n",
    "$$ p(z_i = k \\mid w_{1:K}) \\propto \\exp \\{ x_i^\\top w_k \\}. $$\n",
    "\n",
    "The conditional probability of a marker sequence given subtype membership is\n",
    "\n",
    "$$ p(y_i \\mid z_i = k, \\beta_{1:K}) = \\mathcal{N} ( m_i(k), \\Sigma_i ), $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ m_i(k) = \\Phi_1(t_i) \\Lambda x_i + \\Phi_2(t_i) \\beta_k $$\n",
    "and\n",
    "$$ \\Sigma_i = \\Phi_3(t_i) \\Sigma_b \\Phi_3^\\top(t_i) + K_{\\text{OU}}(t_i) + \\sigma^2 \\mathbf{1}. $$\n",
    "\n",
    "Given some observed data $y_i$, the posterior over subtype membership $z_i$ is\n",
    "\n",
    "$$ p(z_i = k \\mid y_i) \\propto p(z_i = k \\mid w_{1:K}) p(y_i \\mid z_i = k, \\beta_{1:K}). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/pschulam/Git/mypy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-spline Basis\n",
    "\n",
    "This basis is **hard-coded** to implement the exact basis functions used to fit the model in the R code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mypy import bsplines\n",
    "\n",
    "boundaries   = (-1.0, 23.0)\n",
    "degree       = 2\n",
    "num_features = 6\n",
    "basis = bsplines.universal_basis(boundaries, degree, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mypy.util import as_row, as_col\n",
    "\n",
    "def kernel(x1, x2=None, a_const=1.0, a_ou=1.0, l_ou=1.0):\n",
    "    symmetric = x2 is None\n",
    "    d = differences(x1, x1) if symmetric else differences(x1, x2)\n",
    "    K = a_const * np.ones_like(d)\n",
    "    K += ou_kernel(d, a_ou, l_ou)\n",
    "    if symmetric:\n",
    "        K += np.eye(x1.size)\n",
    "    return K\n",
    "\n",
    "def ou_kernel(d, a, l):\n",
    "    return a * np.exp( - np.abs(d) / l )\n",
    "\n",
    "def differences(x1, x2):\n",
    "    return as_col(x1) - as_row(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.linspace(0, 20, 41)\n",
    "X_test = basis.eval(x_test)\n",
    "K_test = kernel(x_test, a_const=16.0, a_ou=36.0, l_ou=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6944,  0.2917,  0.0139,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.5625,  0.4062,  0.0312,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.4444,  0.5   ,  0.0556,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.3403,  0.5729,  0.0868,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.25  ,  0.625 ,  0.125 ,  0.    ,  0.    ,  0.    ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 53.    ,  44.0368,  37.8351,  33.0052,  29.2437],\n",
       "       [ 44.0368,  53.    ,  44.0368,  37.8351,  33.0052],\n",
       "       [ 37.8351,  44.0368,  53.    ,  44.0368,  37.8351],\n",
       "       [ 33.0052,  37.8351,  44.0368,  53.    ,  44.0368],\n",
       "       [ 29.2437,  33.0052,  37.8351,  44.0368,  53.    ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_test[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mypy.models.softmax' from '/Users/pschulam/Git/mypy/mypy/models/softmax.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "from mypy.models import softmax\n",
    "reload(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "a_const = 16.0\n",
    "a_ou    = 36.0\n",
    "l_ou    = 2.0\n",
    "\n",
    "def phi1(x):\n",
    "    return np.ones((x.size, 1))\n",
    "\n",
    "def phi2(x):\n",
    "    return basis.eval(x)\n",
    "\n",
    "def gp_posterior(tnew, t, y, kern, **kwargs):\n",
    "    from numpy import dot\n",
    "    from scipy.linalg import inv, solve\n",
    "    \n",
    "    K11 = kern(tnew, **kwargs)\n",
    "    K12 = kern(tnew, t, **kwargs)\n",
    "    K22 = kern(t, **kwargs)\n",
    "    \n",
    "    m = dot(K12, solve(K22, y))\n",
    "    K = K11 - dot(K12, solve(K22, K12.T))\n",
    "    \n",
    "    return m, K\n",
    "\n",
    "def trajectory_means(t, x, b, B):\n",
    "    from numpy import dot\n",
    "    \n",
    "    P1 = phi1(t)\n",
    "    P2 = phi2(t)\n",
    "    \n",
    "    m1 = dot(P1, dot(b, x)).ravel()\n",
    "    m2 = dot(B, P2.T)\n",
    "    \n",
    "    return m1 + m2\n",
    "\n",
    "def trajectory_logl(t, x, y, z, B, b):\n",
    "    if t.size < 1:\n",
    "        return 0.0\n",
    "    \n",
    "    m = trajectory_means(t, x, b, B)[z]\n",
    "    S = kernel(t, a_const=a_const, a_ou=a_ou, l_ou=l_ou)\n",
    "    \n",
    "    return multivariate_normal.logpdf(y, m, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.loadtxt('param/pop.dat')\n",
    "B = np.loadtxt('param/subpop.dat')\n",
    "W = np.loadtxt('param/marginal.dat')\n",
    "W = np.r_[ np.zeros((1, W.shape[1])), W ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "\n",
    "def model_prior(t, x1, x2, y, b, B, W):\n",
    "    return softmax.regression_log_proba(x2, W)\n",
    "\n",
    "def model_likelihood(t, x1, x2, y, b, B, W):\n",
    "    k = B.shape[0]\n",
    "    return np.array([trajectory_logl(t, x1, y, z, B, b) for z in range(k)])\n",
    "\n",
    "def model_posterior(t, x1, x2, y, b, B, W):\n",
    "    prior = model_prior(t, x1, x2, y, b, B, W)\n",
    "    likel = model_likelihood(t, x1, x2, y, b, B, W)\n",
    "    lp = prior + likel\n",
    "    return np.exp(lp - logsumexp(lp))\n",
    "\n",
    "def model_evidence(t, x1, x2, y, b, B, W):\n",
    "    prior = model_prior(t, x1, x2, y, b, B, W)\n",
    "    likel = model_likelihood(t, x1, x2, y, b, B, W)\n",
    "    lp = prior + likel\n",
    "    return logsumexp(lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def PatientData(tbl):\n",
    "    pd = {}\n",
    "    pd['ptid'] = int(tbl['ptid'].values[0])\n",
    "    pd['t']    = tbl['years_seen_full'].values.copy()\n",
    "    pd['y']    = tbl['pfvc'].values.copy()\n",
    "    pd['x1']   = np.asarray(tbl.loc[:, ['female', 'afram']].drop_duplicates()).ravel()\n",
    "    pd['x2']   = np.asarray(tbl.loc[:, ['female', 'afram', 'aca', 'scl']].drop_duplicates()).ravel()\n",
    "    pd['x2']   = np.r_[ 1.0, pd['x2'] ]\n",
    "    return pd\n",
    "\n",
    "def truncated_data(pd, censor_time):\n",
    "    obs = pd['t'] <= censor_time\n",
    "    pdc = deepcopy(pd)\n",
    "    pdc['t'] = pd['t'][obs]\n",
    "    pdc['y'] = pd['y'][obs]\n",
    "    return pdc, pd['t'][~obs]\n",
    "\n",
    "def eval_prior(pd, b=b, B=B, W=W):\n",
    "    return model_prior(pd['t'], pd['x1'], pd['x2'], pd['y'], b, B, W)\n",
    "\n",
    "def eval_likel(pd, b=b, B=B, W=W):\n",
    "    return model_likelihood(pd['t'], pd['x1'], pd['x2'], pd['y'], b, B, W)\n",
    "\n",
    "def run_inference(pd, b=b, B=B, W=W):\n",
    "    ll = model_loglik(pd['t'], pd['x1'], pd['x2'], pd['y'], b, B, W)\n",
    "    posterior = model_posterior(pd['t'], pd['x1'], pd['x2'], pd['y'], b, B, W)\n",
    "    return ll, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pfvc = pd.read_csv('data/benchmark_pfvc.csv')\n",
    "data = [PatientData(tbl) for _, tbl in pfvc.groupby('ptid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.   ,  0.   ,  0.   ,  0.01 ,  0.022,  0.117,  0.816,  0.035])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll, pst = run_inference(data[9], b, B, W)\n",
    "np.round(pst, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Tuning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to tune the posterior predictions of our model at a given time point by adjusting the relative strengths of the likelihood and the prior used to determine the likelihood ratio that determines the posterior. The goal is to fit the *full information posterior* by modifying the *partial information posterior*. For any observed marker sequence $y_i$, we can express the posterior probabilities by specifying the log of the likelihood ratios of each subtype to some *pivot* subtype.\n",
    "\n",
    "$$ r_{11} = \\log \\frac{p(z = 1)}{p(z = 1)} + \\log \\frac{p(y_i \\mid z = 1)}{p(y_i \\mid z = 1)} $$\n",
    "\n",
    "$$ r_{21} = \\log \\frac{p(z = 2)}{p(z = 1)} + \\log \\frac{p(y_i \\mid z = 2)}{p(y_i \\mid z = 1)} $$\n",
    "\n",
    "$$ r_{31} = \\log \\frac{p(z = 3)}{p(z = 1)} + \\log \\frac{p(y_i \\mid z = 3)}{p(y_i \\mid z = 1)} $$\n",
    "\n",
    "$$ \\ldots $$\n",
    "\n",
    "Note that the first ratio is 0 since it is the log of a ratio that will always be 1. When making a MAP estimate of an individual's subtype, the maximum of these ratios is selected. More generally, if want to match the partial information posterior to the full information posterior as closely as possible, then we want to match these ratios as closely as possible. This suggests a simple adjustment algorithm--- fit $K - 1$ separate regressions where the features are the log ratios of each term in the joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_ratio(L, pivot=0):\n",
    "    R = L - L[:, pivot][:, np.newaxis]\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_log_priors = np.array([eval_prior(d) for d in data])\n",
    "full_log_likels = np.array([eval_likel(d) for d in data])\n",
    "\n",
    "yr01_log_priors = np.array([eval_prior(truncated_data(d, 1.0)[0]) for d in data])\n",
    "yr01_log_likels = np.array([eval_likel(truncated_data(d, 1.0)[0]) for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1 = log_ratio(yr01_log_priors)\n",
    "L2 = log_ratio(yr01_log_likels)\n",
    "Y  = log_ratio(full_log_priors) + log_ratio(full_log_likels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_adjustment(y, x1, x2):\n",
    "    from scipy.linalg import lstsq\n",
    "    n = y.size\n",
    "    X = np.c_[ np.ones(n), x1, x2 ]\n",
    "    w, _, _, _ = lstsq(X, y)\n",
    "    return w\n",
    "\n",
    "def make_adjustment(x1, x2, w):\n",
    "    n = x1.size\n",
    "    X = np.c_[ np.ones(n), x1, x2 ]\n",
    "    return np.dot(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Yhat = np.zeros_like(Y)\n",
    "N, K = Yhat.shape\n",
    "W    = np.zeros((K, 3))\n",
    "for k in range(1, K):\n",
    "    w = fit_adjustment(Y[:, k], L1[:, k], L2[:, k])\n",
    "    W[k] = w\n",
    "    Yhat[:, k] = make_adjustment(L1[:, k], L2[:, k], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_log_ratio = Y\n",
    "yr01_log_ratio = L1 + L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P  = np.array([softmax.softmax_func(y) for y in full_log_ratio])\n",
    "Q1 = np.array([softmax.softmax_func(y) for y in yr01_log_ratio])\n",
    "Q2 = np.array([softmax.softmax_func(y) for y in Yhat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-430.77831436230178"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(P * np.log(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-790.52486194944299"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(P * np.log(Q1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1244.9793742351435"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(P * np.log(Q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple approach doesn't work very well using the multinomial regression objective as an evaluation, but this makes sense because each of the weights are learned entirely independently. Another option for evaluation is to look at whether the MAP under the adjusted distribution agrees more with the MAP under full information that the map under partial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63690476190476186"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(P, axis=1) == np.argmax(Q1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5267857142857143"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(P, axis=1) == np.argmax(Q2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results are not good. This isn't hopeless, though, since the way we trained the adjustment is pretty simple. For completeness, however, let's take a look at the confusion matrix to see if any key mistakes are being corrected using this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 17,   4,   2,   1,   0,   0,   0,   0],\n",
       "       [  3, 103,  21,   7,   4,   1,   0,   0],\n",
       "       [  0,  24,  77,   6,   3,   0,   1,   0],\n",
       "       [  0,  10,  21,  51,  21,   0,   6,   0],\n",
       "       [  0,   0,   2,  26,  93,   7,   5,   3],\n",
       "       [  0,   0,   2,   0,  18,  35,   6,   1],\n",
       "       [  0,   1,   4,  10,  14,   7,  35,   1],\n",
       "       [  0,   0,   0,   1,   1,   0,   0,  17]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(np.argmax(P, axis=1), np.argmax(Q1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 16,  0,  0,  3,  0,  0,  0],\n",
       "       [ 0, 84, 49,  0,  6,  0,  0,  0],\n",
       "       [ 0, 15, 82,  1, 13,  0,  0,  0],\n",
       "       [ 0,  6, 41, 26, 35,  1,  0,  0],\n",
       "       [ 0,  0,  5, 11, 84, 32,  3,  1],\n",
       "       [ 0,  0,  0,  1,  9, 49,  1,  2],\n",
       "       [ 0,  1, 11, 10, 11, 30,  9,  0],\n",
       "       [ 0,  0,  0,  0,  1,  2,  1, 15]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.argmax(P, axis=1), np.argmax(Q2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multinom_pred(W, P, X1, X2):\n",
    "    Z = np.zeros_like(P)\n",
    "    N, K = Z.shape\n",
    "    for k in range(1, K):\n",
    "        w = W[k]\n",
    "        X = np.c_[ np.ones(N), X1[:, k], X2[:, k] ]\n",
    "        Z[:, k] = np.dot(X, w)\n",
    "    \n",
    "    Q = np.array([softmax.softmax_func(z) for z in Z])\n",
    "    return Q\n",
    "\n",
    "def multinom_cost(W, P, X1, X2):\n",
    "    Q = multinom_pred(W, P, X1, X2)\n",
    "    return np.sum(P * np.log(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multinom_grad(W, P, X1, X2):\n",
    "    Z = np.zeros_like(P)\n",
    "    N, K = Z.shape\n",
    "    for k in range(1, K):\n",
    "        w = W[k]\n",
    "        X = np.c_[ np.ones(N), X1[:, k], X2[:, k] ]\n",
    "        Z[:, k] = np.dot(X, w)\n",
    "    Q = np.array([softmax.softmax_func(z) for z in Z])\n",
    "        \n",
    "    D = np.zeros_like(W)\n",
    "    for k in range(1, K):\n",
    "        for i, z in enumerate(Z):\n",
    "            g = softmax.softmax_grad(z)\n",
    "            x = np.r_[ 1.0, X1[i, k], X2[i, k] ]\n",
    "            for j in range(K):\n",
    "                D[k] += P[i, j] / Q[i, j] * g[j, k] * x\n",
    "            \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X1 = StandardScaler().fit_transform(L1)\n",
    "X2 = StandardScaler().fit_transform(L2)\n",
    "W0 = np.zeros_like(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1397.3847160088496"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinom_cost(W0, P, X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00e+00,   0.00e+00,   0.00e+00],\n",
       "       [  5.17e+01,  -2.33e+01,  -1.12e+02],\n",
       "       [  3.35e+01,  -2.44e+01,  -5.99e+01],\n",
       "       [  2.63e+01,   9.03e+00,   4.83e-02],\n",
       "       [  3.78e+01,   2.63e+01,   6.12e+01],\n",
       "       [ -1.52e+01,   4.51e+01,   7.19e+01],\n",
       "       [ -1.26e+01,   2.75e+00,   5.78e+01],\n",
       "       [ -6.35e+01,   6.07e+00,   4.52e+01]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinom_grad(W0, P, X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_grad(f, x0, eps=1e-10):\n",
    "    f0 = f(x0)\n",
    "    n = x0.size\n",
    "    g = np.zeros_like(x0)\n",
    "    for i in range(n):\n",
    "        dt = np.zeros_like(x0)\n",
    "        dt[i] += eps\n",
    "        f1 = f(x0 + dt)\n",
    "        g[i] = (f1 - f0) / eps\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wshape = W0.shape\n",
    "f = lambda w: -multinom_cost(w.reshape(wshape), P, X1, X2)\n",
    "g = lambda w: -multinom_grad(w.reshape(wshape), P, X1, X2).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00e+00,   0.00e+00,   0.00e+00],\n",
       "       [ -5.17e+01,   2.33e+01,   1.13e+02],\n",
       "       [ -3.35e+01,   2.44e+01,   5.99e+01],\n",
       "       [ -2.63e+01,  -9.03e+00,  -4.55e-02],\n",
       "       [ -3.78e+01,  -2.63e+01,  -6.12e+01],\n",
       "       [  1.52e+01,  -4.50e+01,  -7.19e+01],\n",
       "       [  1.26e+01,  -2.75e+00,  -5.78e+01],\n",
       "       [  6.35e+01,  -6.07e+00,  -4.52e+01]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_grad(f, W0.ravel(), 1e-11).reshape(wshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = opt.minimize(f, W.ravel(), jac=g, method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.  ,   0.  ,   0.  ],\n",
       "       [  9.69,   5.01,   4.65],\n",
       "       [ 10.57,   4.87,   6.19],\n",
       "       [ 11.37,   5.23,   8.61],\n",
       "       [ 10.9 ,   5.32,  10.51],\n",
       "       [  8.59,   5.67,  12.58],\n",
       "       [  9.67,   5.12,  11.51],\n",
       "       [ -1.6 ,   8.55,  16.67]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = s.x.reshape(wshape)\n",
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-777.60273430499228"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinom_cost(W2, P, X1, X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a slight improvement in terms of log-likelihood. Let's check the accuracy of MAP subtype estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q3 = multinom_pred(W2, P, X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63690476190476186"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(P, axis=1) == np.argmax(Q1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6339285714285714"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(P, axis=1) == np.argmax(Q3, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement on MAP accuracy, but we're also not directly optimizing for that. Let's try altering the objective function by fitting to the degenerate distribution over subtypes at the MAP estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P2 = np.array([softmax.onehot_encode(np.argmax(p), P.shape[1]) for p in P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wshape = W0.shape\n",
    "f = lambda w: -multinom_cost(w.reshape(wshape), P2, X1, X2)\n",
    "g = lambda w: -multinom_grad(w.reshape(wshape), P2, X1, X2).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397.3847160088499"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -0.  ,   -0.  ,   -0.  ],\n",
       "       [ -55.  ,   21.42,  118.91],\n",
       "       [ -27.  ,   27.64,   62.42],\n",
       "       [ -25.  ,  -13.03,    3.06],\n",
       "       [ -52.  ,  -30.04,  -73.47],\n",
       "       [  22.  ,  -46.03,  -72.72],\n",
       "       [  12.  ,    1.52,  -58.49],\n",
       "       [  65.  ,   -5.31,  -43.18]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(W0).reshape(wshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.  ,    0.  ,    0.  ],\n",
       "       [ -55.  ,   21.42,  118.91],\n",
       "       [ -27.  ,   27.64,   62.41],\n",
       "       [ -25.  ,  -13.03,    3.06],\n",
       "       [ -52.  ,  -30.04,  -73.47],\n",
       "       [  22.  ,  -46.04,  -72.72],\n",
       "       [  12.  ,    1.51,  -58.5 ],\n",
       "       [  65.  ,   -5.31,  -43.18]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_grad(f, W0.ravel()).reshape(wshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = opt.minimize(f, x0=W0.ravel(), jac=g, method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W3 = s2.x.reshape(wshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q4 = multinom_pred(W3, P2, X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63690476190476186"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(P, axis=1) == np.argmax(Q1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6383928571428571"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(P, axis=1) == np.argmax(Q4, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 17,   4,   2,   1,   0,   0,   0,   0],\n",
       "       [  3, 103,  21,   7,   4,   1,   0,   0],\n",
       "       [  0,  24,  77,   6,   3,   0,   1,   0],\n",
       "       [  0,  10,  21,  51,  21,   0,   6,   0],\n",
       "       [  0,   0,   2,  26,  93,   7,   5,   3],\n",
       "       [  0,   0,   2,   0,  18,  35,   6,   1],\n",
       "       [  0,   1,   4,  10,  14,   7,  35,   1],\n",
       "       [  0,   0,   0,   1,   1,   0,   0,  17]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.argmax(P, axis=1), np.argmax(Q1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  4,  2,  1,  0,  0,  0,  0],\n",
       "       [ 4, 96, 27,  7,  4,  1,  0,  0],\n",
       "       [ 0, 20, 77, 11,  3,  0,  0,  0],\n",
       "       [ 0, 10, 16, 59, 19,  0,  5,  0],\n",
       "       [ 0,  0,  1, 24, 96,  6,  8,  1],\n",
       "       [ 0,  0,  2,  0, 20, 33,  6,  1],\n",
       "       [ 0,  1,  2, 11, 14,  8, 36,  0],\n",
       "       [ 0,  0,  0,  1,  1,  2,  0, 15]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.argmax(P, axis=1), np.argmax(Q4, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Directions\n",
    "\n",
    "1. Use a Bayesian multinomial logistic regression classifier to sidestep the inexpressive linear model.\n",
    "\n",
    "2. Change the objective function to more directly reflect the cost function used to evaluate the model (i.e. not all subtype misclassification mistakes are equal, some mistakes are more costly and perhaps we'd like to reflect that in the learning procedure).\n",
    "\n",
    "3. Incorporate additional likelihood ratios based on other longitudinally measured outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Additional Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L3 = np.loadtxt('param/gi_ratios.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X3 = StandardScaler().fit_transform(L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multinom_pred2(W, P, X1, X2, X3):\n",
    "    Z = np.zeros_like(P)\n",
    "    N, K = Z.shape\n",
    "    for k in range(1, K):\n",
    "        w = W[k]\n",
    "        X = np.c_[ np.ones(N), X1[:, k], X2[:, k], X3[:, k] ]\n",
    "        Z[:, k] = np.dot(X, w)\n",
    "    \n",
    "    Q = np.array([softmax.softmax_func(z) for z in Z])\n",
    "    return Q\n",
    "\n",
    "def multinom_cost2(W, P, X1, X2, X3):\n",
    "    Q = multinom_pred2(W, P, X1, X2, X3)\n",
    "    return np.sum(P * np.log(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multinom_grad2(W, P, X1, X2, X3):\n",
    "    Z = np.zeros_like(P)\n",
    "    N, K = Z.shape\n",
    "    for k in range(1, K):\n",
    "        w = W[k]\n",
    "        X = np.c_[ np.ones(N), X1[:, k], X2[:, k], X3[:, k] ]\n",
    "        Z[:, k] = np.dot(X, w)\n",
    "    Q = np.array([softmax.softmax_func(z) for z in Z])\n",
    "        \n",
    "    D = np.zeros_like(W)\n",
    "    for k in range(1, K):\n",
    "        for i, z in enumerate(Z):\n",
    "            g = softmax.softmax_grad(z)\n",
    "            x = np.r_[ 1.0, X1[i, k], X2[i, k], X3[i, k] ]\n",
    "            for j in range(K):\n",
    "                D[k] += P[i, j] / Q[i, j] * g[j, k] * x\n",
    "            \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W0 = np.zeros((P.shape[1], 4))\n",
    "f2 = lambda w: -multinom_cost2(w.reshape(W0.shape), P, X1, X2, X3)\n",
    "g2 = lambda w: -multinom_grad2(w.reshape(W0.shape), P, X1, X2, X3).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = opt.minimize(f2, W0.ravel(), jac=g2, method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q5 = multinom_pred2(s2.x.reshape(W0.shape), P, X1, X2, X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6607142857142857"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(P, axis=1) == np.argmax(Q5, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
